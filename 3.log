loader/checkpoint.go:139:2: err := cp.conn.executeSQL(tctx, []string{sql2})
loader/checkpoint.go:159:2: err := cp.conn.executeSQL(tctx, []string{sql2})
loader/checkpoint.go:173:8: rows, err := cp.conn.querySQL(tctx, query, cp.id)
loader/checkpoint.go:178:18: defer rows.Close()
loader/checkpoint.go:350:2: err = cp.conn.executeSQL(tctx, []string{sql2}, args)
loader/checkpoint.go:413:2: err := cp.conn.executeSQL(tctx, []string{sql2})
loader/checkpoint.go:422:8: rows, err := cp.conn.querySQL(tctx, query, cp.id)
loader/checkpoint.go:427:18: defer rows.Close()
loader/convert_data.go:193:16: defer fd.Close()
loader/db.go:152:7: _, err := conn.baseConn.ExecuteSQL(ctx, stmtHistogram, conn.cfg.Name, queries, args...)
loader/loader.go:197:4: err := w.conn.executeSQL(ctctx, sqls)
loader/loader.go:204:5: err = terror.WithScope(terror.Annotatef(err, "file %s", job.file), terror.ScopeDownstream)
loader/loader.go:241:5: err = terror.Annotatef(err, "restore data file (%v) failed", job.dataFile)
loader/loader.go:286:15: defer f.Close()
loader/loader.go:298:3: err2 = w.checkPoint.Init(tctx, baseFile, finfo.Size())
loader/loader.go:331:9: line, err := br.ReadString('n')
loader/loader.go:473:14: checkpoint, err := newRemoteCheckPoint(tctx, l.cfg, l.checkpointID())
loader/loader.go:565:2: err := l.Restore(newCtx)
loader/loader.go:712:2: err = l.restoreData(ctx)
loader/loader.go:889:17: l.tableRouter, _ = router.NewTableRouter(l.cfg.CaseSensitive, []*router.TableRule{})
loader/loader.go:1121:15: defer f.Close()
loader/loader.go:1235:3: err = q.ctx.Err()
loader/loader.go:1341:3: runtimeErr := dbRestoreQueue.close()
loader/loader.go:1373:6: err = terror.Annotatef(err, "parse table %s/%s", db, table)
loader/loader.go:1395:3: runtimeErr := tblRestoreQueue.close()
loader/util.go:83:18: defer file.Close()
monitoring/dashboards/dashboard.go:52:15: defer f.Close()
pkg/binlog/event/common.go:118:11: gtidEv, err = GenMariaDBGTIDEvent(header, latestPos, mariaGTID.SequenceNumber, mariaGTID.DomainID)
pkg/binlog/event/common.go:149:3: err = clone.Set(gtidSet)
pkg/binlog/reader/tcp.go:146:17: defer db.Close()
pkg/binlog/reader/util.go:84:7: u, _ := uuid.FromBytes(ev.SID)
pkg/binlog/reader/util.go:145:15: defer r.Close()
pkg/binlog/reader/util.go:157:15: defer r.Close()
pkg/binlog/writer/file.go:116:3: err = w.file.Close()
pkg/binlog/writer/file.go:133:5: n, err := w.file.Write(rawData)
pkg/conn/basedb.go:108:11: db.Close()
pkg/conn/basedb.go:109:15: db, mockDB, _ = sqlmock.New()
pkg/conn/basedb.go:116:2: err = db.PingContext(ctx)
pkg/conn/basedb.go:121:11: db.Close()
pkg/conn/basedb.go:185:3: terr := conn.close()
pkg/conn/basedb.go:190:2: terr := d.DB.Close()
pkg/dumpling/utils.go:40:16: defer fd.Close()
pkg/election/election.go:236:5: err = terror.ErrElectionCampaignFail.Delegate(err, "create a new session")
pkg/gtid/gtid.go:69:10: gtid, err = mysql.ParseGTIDSet(fla, gtidStr)
pkg/ha/bound.go:64:5: s, _ := b.toJSON()
pkg/ha/bound.go:85:2: err = json.Unmarshal([]byte(s), &b)
pkg/ha/bound.go:146:9: resp, err = cli.Get(ctx, common.UpstreamBoundWorkerKeyAdapter.Encode(worker))
pkg/ha/bound.go:292:13: bound, err = sourceBoundFromJSON(string(ev.Kv.Value))
pkg/ha/bound.go:294:13: bound, err = sourceBoundFromKey(string(ev.Kv.Key))
pkg/ha/keepalive.go:50:7: str, _ := w.toJSON()
pkg/ha/keepalive.go:65:2: err = json.Unmarshal([]byte(s), &w)
pkg/ha/keepalive.go:225:13: event, err = workerEventFromKey(string(ev.Kv.Key))
pkg/ha/load_task.go:131:8: err = json.Unmarshal(ev.Kv.Value, &loadTask.Worker)
pkg/ha/relay.go:139:20: newSource, rev2, err = getSourceIDFromResp((*clientv3.GetResponse)(sourceResp))
pkg/ha/source.go:58:9: resp, err = cli.Get(ctx, common.UpstreamConfigKeyAdapter.Encode(source), clientv3.WithRev(rev))
pkg/ha/stage.go:66:7: str, _ := s.toJSON()
pkg/ha/stage.go:87:2: err = json.Unmarshal([]byte(str), &s)
pkg/ha/stage.go:184:9: resp, err = cli.Get(ctx, common.StageSubTaskKeyAdapter.Encode(source, task))
pkg/ha/stage.go:354:13: stage, err = stageFromJSON(string(ev.Kv.Value))
pkg/ha/stage.go:356:13: stage, err = stageFromKey(string(ev.Kv.Key))
pkg/ha/subtask.go:41:9: resp, err = cli.Get(ctx, common.UpstreamSubTaskKeyAdapter.Encode(source, task), clientv3.WithRev(rev))
pkg/ha/worker.go:42:5: s, _ := i.toJSON()
pkg/ha/worker.go:57:2: err = json.Unmarshal([]byte(s), &i)
pkg/parser/common.go:39:19: stmts, warnings, err := p.Parse(sql, charset, collation)
pkg/shardddl/optimism/info.go:102:5: s, _ := i.toJSON()
pkg/shardddl/optimism/info.go:127:5: s, _ := logInfo.toJSON()
pkg/shardddl/optimism/info.go:253:12: info, err = infoFromJSON(string(ev.Kv.Value))
pkg/shardddl/optimism/info.go:257:12: info, err = infoFromJSON(string(ev.PrevKv.Value))
pkg/shardddl/optimism/info.go:327:2: err = json.Unmarshal([]byte(s), &oldInfo)
pkg/shardddl/optimism/lock.go:322:8: cmp, _ = prevTable.Compare(nextTable) // we have checked `err` returned above.
pkg/shardddl/optimism/operation.go:83:5: s, _ := o.toJSON()
pkg/shardddl/optimism/operation.go:98:2: err = json.Unmarshal([]byte(s), &o)
pkg/shardddl/optimism/schema.go:48:5: s, _ := is.toJSON()
pkg/shardddl/optimism/schema.go:69:2: err = json.Unmarshal([]byte(s), &is)
pkg/shardddl/optimism/table.go:94:5: s, _ := st.toJSON()
pkg/shardddl/optimism/table.go:180:2: err = json.Unmarshal([]byte(s), &st)
pkg/shardddl/optimism/table.go:262:10: st, err = sourceTablesFromKey(string(ev.Kv.Key))
pkg/shardddl/pessimism/info.go:53:5: s, _ := i.toJSON()
pkg/shardddl/pessimism/info.go:68:2: err = json.Unmarshal([]byte(s), &i)
pkg/shardddl/pessimism/info.go:120:12: opBefore, err := operationFromJSON(string(opsResp.Kvs[0].Value))
pkg/shardddl/pessimism/operation.go:58:5: s, _ := o.toJSON()
pkg/shardddl/pessimism/operation.go:73:2: err = json.Unmarshal([]byte(s), &o)
pkg/shardddl/pessimism/operation.go:255:10: op, err = operationFromJSON(string(ev.Kv.Value))
pkg/shardddl/pessimism/operation.go:257:10: op, err = operationFromJSON(string(ev.PrevKv.Value))
pkg/streamer/file.go:161:17: defer dir.Close()
pkg/streamer/file.go:215:3: err = terror.ErrAddWatchForRelayLogDir.Delegate(err, dir)
pkg/streamer/file.go:221:3: err = terror.ErrWatcherStart.Delegate(err, dir)
pkg/streamer/file.go:289:3: err = terror.Annotatef(err, "collect newer files from %s in dir %s", latestFile, dir)
pkg/streamer/file.go:294:7: cmp, err := fileSizeUpdated(latestFilePath, latestFileSize)
pkg/streamer/file.go:299:3: err = terror.ErrRelayLogFileSizeSmaller.Generate(latestFilePath)
pkg/streamer/reader.go:124:24: defer fileReader.Close()
pkg/streamer/reader.go:140:6: e, err := fileReader.GetEvent(ctx2)
pkg/streamer/reader.go:153:8: gs, err = event.GTIDsFromPreviousGTIDsEvent(e)
pkg/streamer/reader.go:379:53: needSwitch, latestPos, nextUUID, nextBinlogName, err = r.parseFileAsPossible(ctx, s, relayLogFile, offset, dir, firstParse, currentUUID, i == len(files)-1)
pkg/streamer/reader.go:410:87: needSwitch, needReParse, latestPos, nextUUID, nextBinlogName, replaceWithHeartbeat, err = r.parseFile(ctx, s, relayLogFile, latestPos, relayLogDir, firstParse, currentUUID, possibleLast, replaceWithHeartbeat)
pkg/streamer/reader.go:453:12: parsed, _ := binlog.ParseFilename(string(ev.NextLogName))
pkg/streamer/reader.go:478:7: u, _ := uuid.FromBytes(ev.SID)
pkg/terror/terror.go:224:14: fmt.Fprintf(s, "%q", e.Error())
pkg/upgrade/upgrade.go:67:13: preVer, _, err := GetVersion(cli)
pkg/upgrade/upgrade.go:109:13: preVer, _, err := GetVersion(cli)
pkg/upgrade/upgrade.go:189:12: db.Close()
pkg/upgrade/version.go:76:5: s, _ := v.toJSON()
pkg/upgrade/version.go:91:2: err = json.Unmarshal([]byte(s), &v)
pkg/utils/common.go:58:11: schemas, err := dbutil.GetSchemas(ctx, db)
pkg/utils/common.go:118:16: sourceTables, err := FetchAllDoTables(ctx, db, bw)
pkg/utils/db.go:110:18: defer rows.Close()
pkg/utils/db.go:147:4: err = rows.Scan(&serverID, &host, &port, &masterID, &slaveUUID)
pkg/utils/db.go:183:18: defer rows.Close()
pkg/utils/db.go:224:3: err = rows.Scan(&binlogName, &pos, &nullPtr, &nullPtr, &gtidStr)
pkg/utils/db.go:299:18: defer conn.Close()
pkg/utils/db.go:414:15: defer c.Close()
pkg/utils/db.go:447:2: err = errors.Cause(err)
pkg/utils/db.go:524:2: _ = ret.Set(newGset)
pkg/utils/file.go:83:5: n, err := f.Write(data)
pkg/utils/file.go:84:9: f.Close()
pkg/utils/relay.go:45:16: defer fd.Close()
pkg/utils/util.go:151:2: err = errors.Cause(err) // check the original error
pkg/v1dbschema/schema.go:98:8: pos, err := getGlobalPos(tctx, dbConn, tableName, sourceID)
pkg/v1dbschema/schema.go:161:18: defer rows.Close()
pkg/v1dbschema/schema.go:187:7: gs, _ = gtid.ParserGTID(gmysql.MySQLFlavor, str)
pkg/v1dbschema/schema.go:215:2: err = errors.Cause(err) // check the original error
pkg/v1workermeta/api.go:62:16: defer db.Close()
pkg/v1workermeta/api.go:87:16: defer db.Close()
relay/meta.go:118:16: lm.emptyGSet, _ = gtid.ParserGTID(flavor, "")
relay/meta.go:161:14: _, suffix, _ := utils.ParseSuffixForUUID(lm.currentUUID)
relay/meta.go:466:16: defer fd.Close()
relay/purger/strategy_filename.go:47:16: _, endSuffix, _ := utils.ParseSuffixForUUID(uuid)
relay/purger/strategy_filename.go:65:14: _, suffix, _ := utils.ParseSuffixForUUID(uuid)
relay/reader/reader.go:103:3: err = r.setUpReaderByGTID()
relay/reader/reader.go:120:2: err := r.in.Close()
relay/reader/reader.go:138:7: ev, err := r.in.GetEvent(ctx2)
relay/relay.go:328:5: d, err := os.Open(dir)
relay/relay.go:337:15: defer d.Close()
relay/relay.go:397:23: defer dbConn.Close()
relay/relay.go:695:12: pos, _, err := utils.GetMasterStatus(ctx2, r.db.DB, r.cfg.Flavor)
relay/relay.go:815:13: r.db.Close()
relay/relay.go:941:13: _, suffix, _ := utils.ParseSuffixForUUID(uuid)
relay/relay.go:1019:20: defer dbConn.Close()
relay/writer/file.go:97:3: err = w.out.Close()
relay/writer/file.go:398:6: fs, err := os.Stat(filename)
relay/writer/file.go:427:15: defer f.Close()
relay/writer/file_util.go:43:15: defer f.Close()
relay/writer/file_util.go:76:15: defer f.Close()
relay/writer/file_util.go:114:7: eof, err := replication.NewBinlogParser().ParseSingleEvent(f, onEventFunc)
relay/writer/file_util.go:151:15: defer f.Close()
relay/writer/file_util.go:175:15: defer r.Close()
relay/writer/file_util.go:190:6: e, err = r.GetEvent(ctx2)
relay/writer/file_util.go:224:7: u, _ := uuid.FromBytes(ev.SID)
syncer/checkpoint.go:118:13: trackedTi, _ := schemaTracker.GetTable(schema, b.ti.Name.O) // ignore the returned error, only compare `trackerTi` is enough.
syncer/checkpoint.go:698:5: _, err := cp.dbConn.executeSQL(tctx, []string{sql2}, [][]interface{}{args}...)
syncer/checkpoint.go:722:5: _, err := cp.dbConn.executeSQL(tctx, sqls)
syncer/checkpoint.go:733:8: rows, err := cp.dbConn.querySQL(tctx, query, cp.id)
syncer/checkpoint.go:736:14: rows.Close()
syncer/db.go:81:16: pos, gtidSet, err := utils.GetMasterStatus(ctx, conn.BaseDB.DB, flavor)
syncer/db.go:347:18: defer rows.Close()
syncer/db.go:359:4: err = rows.Scan(&file, &pos)
syncer/online_ddl.go:140:18: defer rows.Close()
syncer/relay.go:94:2: err = s.readerHub.UpdateActiveRelayLog(s.cfg.Name, activeUUID, pos.Name)
syncer/relay.go:124:2: err = s.readerHub.UpdateActiveRelayLog(s.cfg.Name, activeUUID, pos.Name)
syncer/sharding_group.go:437:39: needShardingHandle, synced, remain, err = k.groups[targetTableID].Merge(sourceIDs)
syncer/sharding_group.go:439:3: err = terror.ErrSyncUnitDupTableGroup.Generate(targetTableID)
syncer/sharding_group.go:688:5: _, err := k.dbConn.executeSQL(k.tctx, []string{stmt})
syncer/sharding_group.go:705:5: _, err := k.dbConn.executeSQL(k.tctx, []string{stmt})
syncer/sharding_group.go:717:18: defer rows.Close()
syncer/streamer_controller.go:148:3: err = c.resetReplicationSyncer(tctx, location)
syncer/streamer_controller.go:251:9: event, err = streamer.GetEvent(ctx)
