pkg/shardddl/optimism/info.go:102:5: s, _ := i.toJSON()
pkg/shardddl/optimism/info.go:127:5: s, _ := logInfo.toJSON()
pkg/shardddl/optimism/info.go:253:12: info, err = infoFromJSON(string(ev.Kv.Value))
pkg/shardddl/optimism/info.go:257:12: info, err = infoFromJSON(string(ev.PrevKv.Value))
pkg/shardddl/optimism/info.go:327:2: err = json.Unmarshal([]byte(s), &oldInfo)
pkg/shardddl/optimism/lock.go:322:8: cmp, _ = prevTable.Compare(nextTable) // we have checked `err` returned above.
pkg/shardddl/optimism/operation.go:83:5: s, _ := o.toJSON()
pkg/shardddl/optimism/operation.go:98:2: err = json.Unmarshal([]byte(s), &o)
pkg/shardddl/optimism/schema.go:48:5: s, _ := is.toJSON()
pkg/shardddl/optimism/schema.go:69:2: err = json.Unmarshal([]byte(s), &is)
pkg/shardddl/optimism/table.go:94:5: s, _ := st.toJSON()
pkg/shardddl/optimism/table.go:180:2: err = json.Unmarshal([]byte(s), &st)
pkg/shardddl/optimism/table.go:262:10: st, err = sourceTablesFromKey(string(ev.Kv.Key))
pkg/shardddl/pessimism/info.go:53:5: s, _ := i.toJSON()
pkg/shardddl/pessimism/info.go:68:2: err = json.Unmarshal([]byte(s), &i)
pkg/shardddl/pessimism/info.go:120:12: opBefore, err := operationFromJSON(string(opsResp.Kvs[0].Value))
pkg/shardddl/pessimism/operation.go:58:5: s, _ := o.toJSON()
pkg/shardddl/pessimism/operation.go:73:2: err = json.Unmarshal([]byte(s), &o)
pkg/shardddl/pessimism/operation.go:255:10: op, err = operationFromJSON(string(ev.Kv.Value))
pkg/shardddl/pessimism/operation.go:257:10: op, err = operationFromJSON(string(ev.PrevKv.Value))
pkg/streamer/file.go:161:17: defer dir.Close()
pkg/streamer/file.go:215:3: err = terror.ErrAddWatchForRelayLogDir.Delegate(err, dir)
pkg/streamer/file.go:221:3: err = terror.ErrWatcherStart.Delegate(err, dir)
pkg/streamer/file.go:289:3: err = terror.Annotatef(err, "collect newer files from %s in dir %s", latestFile, dir)
pkg/streamer/file.go:294:7: cmp, err := fileSizeUpdated(latestFilePath, latestFileSize)
pkg/streamer/file.go:299:3: err = terror.ErrRelayLogFileSizeSmaller.Generate(latestFilePath)
pkg/streamer/reader.go:124:24: defer fileReader.Close()
pkg/streamer/reader.go:140:6: e, err := fileReader.GetEvent(ctx2)
pkg/streamer/reader.go:153:8: gs, err = event.GTIDsFromPreviousGTIDsEvent(e)
pkg/streamer/reader.go:379:53: needSwitch, latestPos, nextUUID, nextBinlogName, err = r.parseFileAsPossible(ctx, s, relayLogFile, offset, dir, firstParse, currentUUID, i == len(files)-1)
pkg/streamer/reader.go:410:87: needSwitch, needReParse, latestPos, nextUUID, nextBinlogName, replaceWithHeartbeat, err = r.parseFile(ctx, s, relayLogFile, latestPos, relayLogDir, firstParse, currentUUID, possibleLast, replaceWithHeartbeat)
pkg/streamer/reader.go:453:12: parsed, _ := binlog.ParseFilename(string(ev.NextLogName))
pkg/streamer/reader.go:478:7: u, _ := uuid.FromBytes(ev.SID)
pkg/terror/terror.go:224:14: fmt.Fprintf(s, "%q", e.Error())
pkg/upgrade/upgrade.go:67:13: preVer, _, err := GetVersion(cli)
pkg/upgrade/upgrade.go:109:13: preVer, _, err := GetVersion(cli)
pkg/upgrade/upgrade.go:189:12: db.Close()
pkg/upgrade/version.go:76:5: s, _ := v.toJSON()
pkg/upgrade/version.go:91:2: err = json.Unmarshal([]byte(s), &v)
pkg/utils/common.go:58:11: schemas, err := dbutil.GetSchemas(ctx, db)
pkg/utils/common.go:118:16: sourceTables, err := FetchAllDoTables(ctx, db, bw)
pkg/utils/db.go:110:18: defer rows.Close()
pkg/utils/db.go:147:4: err = rows.Scan(&serverID, &host, &port, &masterID, &slaveUUID)
pkg/utils/db.go:183:18: defer rows.Close()
pkg/utils/db.go:224:3: err = rows.Scan(&binlogName, &pos, &nullPtr, &nullPtr, &gtidStr)
pkg/utils/db.go:299:18: defer conn.Close()
pkg/utils/db.go:414:15: defer c.Close()
pkg/utils/db.go:447:2: err = errors.Cause(err)
pkg/utils/db.go:524:2: _ = ret.Set(newGset)
pkg/utils/file.go:83:5: n, err := f.Write(data)
pkg/utils/file.go:84:9: f.Close()
pkg/utils/relay.go:45:16: defer fd.Close()
pkg/utils/util.go:151:2: err = errors.Cause(err) // check the original error
pkg/v1dbschema/schema.go:98:8: pos, err := getGlobalPos(tctx, dbConn, tableName, sourceID)
pkg/v1dbschema/schema.go:161:18: defer rows.Close()
pkg/v1dbschema/schema.go:187:7: gs, _ = gtid.ParserGTID(gmysql.MySQLFlavor, str)
pkg/v1dbschema/schema.go:215:2: err = errors.Cause(err) // check the original error
pkg/v1workermeta/api.go:62:16: defer db.Close()
pkg/v1workermeta/api.go:87:16: defer db.Close()
relay/meta.go:118:16: lm.emptyGSet, _ = gtid.ParserGTID(flavor, "")
relay/meta.go:161:14: _, suffix, _ := utils.ParseSuffixForUUID(lm.currentUUID)
relay/meta.go:466:16: defer fd.Close()
relay/purger/strategy_filename.go:47:16: _, endSuffix, _ := utils.ParseSuffixForUUID(uuid)
relay/purger/strategy_filename.go:65:14: _, suffix, _ := utils.ParseSuffixForUUID(uuid)
relay/reader/reader.go:103:3: err = r.setUpReaderByGTID()
relay/reader/reader.go:120:2: err := r.in.Close()
relay/reader/reader.go:138:7: ev, err := r.in.GetEvent(ctx2)
relay/relay.go:328:5: d, err := os.Open(dir)
relay/relay.go:337:15: defer d.Close()
relay/relay.go:397:23: defer dbConn.Close()
relay/relay.go:695:12: pos, _, err := utils.GetMasterStatus(ctx2, r.db.DB, r.cfg.Flavor)
relay/relay.go:815:13: r.db.Close()
relay/relay.go:941:13: _, suffix, _ := utils.ParseSuffixForUUID(uuid)
relay/relay.go:1019:20: defer dbConn.Close()
relay/writer/file.go:97:3: err = w.out.Close()
relay/writer/file.go:398:6: fs, err := os.Stat(filename)
relay/writer/file.go:427:15: defer f.Close()
relay/writer/file_util.go:43:15: defer f.Close()
relay/writer/file_util.go:76:15: defer f.Close()
relay/writer/file_util.go:114:7: eof, err := replication.NewBinlogParser().ParseSingleEvent(f, onEventFunc)
relay/writer/file_util.go:151:15: defer f.Close()
relay/writer/file_util.go:175:15: defer r.Close()
relay/writer/file_util.go:190:6: e, err = r.GetEvent(ctx2)
relay/writer/file_util.go:224:7: u, _ := uuid.FromBytes(ev.SID)
syncer/checkpoint.go:118:13: trackedTi, _ := schemaTracker.GetTable(schema, b.ti.Name.O) // ignore the returned error, only compare `trackerTi` is enough.
syncer/checkpoint.go:698:5: _, err := cp.dbConn.executeSQL(tctx, []string{sql2}, [][]interface{}{args}...)
syncer/checkpoint.go:722:5: _, err := cp.dbConn.executeSQL(tctx, sqls)
syncer/checkpoint.go:733:8: rows, err := cp.dbConn.querySQL(tctx, query, cp.id)
syncer/checkpoint.go:736:14: rows.Close()
syncer/db.go:81:16: pos, gtidSet, err := utils.GetMasterStatus(ctx, conn.BaseDB.DB, flavor)
syncer/db.go:347:18: defer rows.Close()
syncer/db.go:359:4: err = rows.Scan(&file, &pos)
syncer/online_ddl.go:140:18: defer rows.Close()
syncer/relay.go:94:2: err = s.readerHub.UpdateActiveRelayLog(s.cfg.Name, activeUUID, pos.Name)
syncer/relay.go:124:2: err = s.readerHub.UpdateActiveRelayLog(s.cfg.Name, activeUUID, pos.Name)
syncer/sharding_group.go:437:39: needShardingHandle, synced, remain, err = k.groups[targetTableID].Merge(sourceIDs)
syncer/sharding_group.go:439:3: err = terror.ErrSyncUnitDupTableGroup.Generate(targetTableID)
syncer/sharding_group.go:688:5: _, err := k.dbConn.executeSQL(k.tctx, []string{stmt})
syncer/sharding_group.go:705:5: _, err := k.dbConn.executeSQL(k.tctx, []string{stmt})
syncer/sharding_group.go:717:18: defer rows.Close()
syncer/streamer_controller.go:148:3: err = c.resetReplicationSyncer(tctx, location)
syncer/streamer_controller.go:251:9: event, err = streamer.GetEvent(ctx)
